# AUTOGENERATED! DO NOT EDIT! File to edit: 01_embedding.ipynb (unless otherwise specified).

__all__ = ['Net', 'emb', 'input', 'model', 'optim', 'w', 'emp', 'input']

# Cell
import torch
import torch.nn as nn

emb = nn.Embedding(10, 3)

list(emb.parameters())

input = torch.tensor([1])

emb(input)

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.to_learn = nn.Embedding(10, 5)
        self.to_comapre_with = nn.Embedding(10, 5)

    def forward(self, input):
        return self.to_learn(input.long()) + self.to_comapre_with(input.long())

model = Net()

list(model.parameters())

model.to_comapre_with.requires_grad_(requires_grad=False)

list(model.parameters())

optim = torch.optim.SGD(model.parameters(), lr=0.01)

for i in range(1000):
    input = torch.randperm(10).float()
    input.requires_grad = True
    optim.zero_grad()

    loss = model(input).abs().sum()
    loss.backward()

    optim.step()
    print(loss)

list(model.parameters())

list(model.parameters())[0]

w = torch.FloatTensor([[1, 2, 3, 4], [4, 5, 6, 7]])

emp = nn.Embedding.from_pretrained(w)

list(emp.parameters())

input = torch.LongTensor([1])

emp(input)